# LiteLLM Model Parameters Reference

**Target Audience:** AI Agents / Claude Code
**Last Updated:** 2025-11-29
**LiteLLM Version:** main-stable
**Data Source:** Live analysis of 17 OpenAI models + official documentation

---

## Quick Reference

### Model Structure (Required Schema)

```json
{
  "model_name": "string (required)",
  "litellm_params": {
    "model": "string (required)",
    "custom_llm_provider": "string (optional)",
    "api_base": "string (optional)",
    "api_key": "string (optional)",
    "tags": ["string"] (optional)
  },
  "model_info": {
    "litellm_provider": "string (required)",
    "mode": "string (required)",
    "max_tokens": "integer (optional)",
    // ... all other fields optional
  }
}
```

### API Endpoints

```
GET  /model/info              # List all models
POST /model/new               # Add model (no restart needed)
POST /model/delete            # Delete model by UUID
```

---

## Section 1: litellm_params (Connection Configuration)

All parameters passed to `litellm.completion()`. Controls how LiteLLM connects to the provider.

### Connection Parameters

```python
{
  # REQUIRED
  "model": str,  # Model identifier sent to provider
                 # Examples: "gpt-4", "azure/gpt-4o", "ollama/llama3:8b"

  # OPTIONAL - Provider Configuration
  "custom_llm_provider": str,  # "openai" | "azure" | "anthropic" | "bedrock" | "ollama"
  "api_base": str,             # Custom endpoint URL
  "api_key": str,              # Auth key (supports "os.environ/VAR_NAME")
  "api_version": str,          # API version (mainly Azure)

  # OPTIONAL - Behavior Flags
  "use_in_pass_through": bool,              # Default: false
  "use_litellm_proxy": bool,                # Default: false
  "merge_reasoning_content_in_choices": bool,  # Default: false (for o1/o4 models)

  # OPTIONAL - Generation Control
  "temperature": float,         # 0.0 - 2.0
  "max_tokens": int,           # Max output tokens
  "top_p": float,              # 0.0 - 1.0
  "frequency_penalty": float,  # -2.0 - 2.0
  "presence_penalty": float,   # -2.0 - 2.0
  "seed": int,                 # For reproducibility
  "n": int,                    # Number of completions
  "stop": str | list[str],     # Stop sequences

  # OPTIONAL - Rate Limiting
  "rpm": int,  # Requests per minute
  "tpm": int,  # Tokens per minute

  # OPTIONAL - Advanced Auth
  "organization": str,         # OpenAI org ID
  "azure_ad_token": str,       # Azure AD token
  "aws_region_name": str,      # AWS region for Bedrock

  # OPTIONAL - HTTP Config
  "extra_headers": dict,       # Custom HTTP headers
  "max_retries": int,          # Retry count

  # OPTIONAL - Organization
  "tags": list[str]            # For filtering/grouping
}
```

### Important Notes on litellm_params

1. **Model Naming Convention:**
   - OpenAI: `"gpt-4"`
   - Azure: `"azure/deployment-name"`
   - Ollama: `"ollama/model-name:tag"`
   - Bedrock: `"bedrock/anthropic.claude-v2"`
   - Custom OpenAI-compatible: `"openai/model-name"` with `api_base`

2. **Environment Variables:**
   ```json
   "api_key": "os.environ/MY_API_KEY"
   ```

3. **Tags for Organization:**
   ```json
   "tags": ["lupdater", "provider:ollama", "type:chat", "env:prod"]
   ```

---

## Section 2: model_info (Model Metadata)

All fields are OPTIONAL unless specified. This section defines model capabilities, limits, and costs.

### Core Metadata

```python
{
  # Identification (auto-generated if db_model=true)
  "id": str,           # UUID (auto-generated by LiteLLM)
  "db_model": bool,    # True if stored in database
  "key": str,          # Model key identifier

  # REQUIRED for functionality
  "litellm_provider": str,  # "openai" | "anthropic" | "ollama" | etc.
  "mode": str,              # See "Model Modes" section below

  # Version tracking
  "version": str,  # Config version
}
```

### Model Modes (mode field)

```python
MODE_TYPES = {
  "chat": "Conversational models (GPT-4, Claude, Llama)",
  "completion": "Text completion models",
  "embedding": "Vector embedding models",
  "audio_transcription": "Speech-to-text (Whisper)",
  "audio_speech": "Text-to-speech (TTS)",
  "image_generation": "Image generation (DALL-E)",
  "moderation": "Content moderation",
  "responses": "Advanced reasoning models (o1, o4)"
}
```

### Token Limits

```python
{
  "max_tokens": int,         # Total context window (input + output)
  "max_input_tokens": int,   # Max input tokens
  "max_output_tokens": int,  # Max output tokens
  "context_window": int,     # Alias for max_tokens
}
```

**Usage Pattern:**
- Chat models: Set all three (max_tokens, max_input_tokens, max_output_tokens)
- Embedding models: Only set max_input_tokens
- Audio models: May not use token limits (use seconds instead)

### Capabilities (Boolean Flags)

All default to `null` if not applicable.

```python
{
  # Core Features
  "supports_system_messages": bool,      # System prompts
  "supports_response_schema": bool,      # Structured outputs (JSON schema)
  "supports_native_streaming": bool,     # Streaming responses

  # Advanced Features
  "supports_function_calling": bool,     # Function/tool calling
  "supports_tool_choice": bool,          # Specify which tool to use
  "supports_assistant_prefill": bool,    # Pre-fill assistant response
  "supports_prompt_caching": bool,       # Prompt caching for cost reduction

  # Multimodal
  "supports_vision": bool,               # Image input
  "supports_audio_input": bool,          # Audio input
  "supports_audio_output": bool,         # Audio output
  "supports_pdf_input": bool,            # PDF processing
  "supports_embedding_image_input": bool, # Images in embeddings

  # Special Capabilities
  "supports_web_search": bool,           # Built-in web search
  "supports_url_context": bool,          # URL fetching
  "supports_reasoning": bool,            # Explicit reasoning (o1/o4)
  "supports_computer_use": bool,         # Computer use API
}
```

### Pricing - Input Costs

All costs in USD per token (float, scientific notation).

```python
{
  # Standard Input Costs
  "input_cost_per_token": float,         # Base rate
  "input_cost_per_token_flex": float,    # OpenAI flex tier
  "input_cost_per_token_priority": float, # OpenAI priority tier
  "input_cost_per_token_batches": float, # Batch API rate

  # Context-Based Input Costs
  "input_cost_per_token_above_128k_tokens": float,
  "input_cost_per_token_above_200k_tokens": float,

  # Alternative Billing Units
  "input_cost_per_character": float,     # TTS models
  "input_cost_per_second": float,        # Audio models
  "input_cost_per_query": float,         # Fixed per query
  "input_cost_per_audio_token": float,   # Audio-specific
}
```

### Pricing - Cache Costs

```python
{
  # Cache Creation
  "cache_creation_input_token_cost": float,
  "cache_creation_input_token_cost_above_200k_tokens": float,
  "cache_creation_input_token_cost_above_1hr": float,

  # Cache Read
  "cache_read_input_token_cost": float,
  "cache_read_input_token_cost_above_200k_tokens": float,
  "cache_read_input_token_cost_flex": float,
  "cache_read_input_token_cost_priority": float,
}
```

### Pricing - Output Costs

```python
{
  # Standard Output Costs
  "output_cost_per_token": float,
  "output_cost_per_token_flex": float,
  "output_cost_per_token_priority": float,
  "output_cost_per_token_batches": float,

  # Context-Based Output Costs
  "output_cost_per_token_above_128k_tokens": float,
  "output_cost_per_token_above_200k_tokens": float,

  # Alternative Billing Units
  "output_cost_per_character": float,
  "output_cost_per_character_above_128k_tokens": float,
  "output_cost_per_reasoning_token": float,  # o1/o4 models
  "output_cost_per_audio_token": float,
  "output_cost_per_second": float,           # Audio generation
  "output_cost_per_image": float,            # Image generation
  "output_cost_per_video_per_second": float,
}
```

### Pricing - Other

```python
{
  "search_context_cost_per_query": float,
  "citation_cost_per_token": float,
  "ocr_cost_per_page": float,
  "annotation_cost_per_page": float,
  "tiered_pricing": dict,  # Complex pricing structures
}
```

**Cost Examples:**
- `1.5e-06` = $0.0000015 per token = $1.50 per million tokens
- `2.5e-05` = $0.000025 per token = $25 per million tokens
- `0.04` = $0.04 per unit (e.g., per image)

### Embeddings

```python
{
  "output_vector_size": int,  # Embedding dimensions (e.g., 3072)
}
```

### Rate Limiting

```python
{
  "rpm": int,  # Requests per minute limit
  "tpm": int,  # Tokens per minute limit
}
```

### Access Control

```python
{
  "access_groups": list[str],        # ["engineering", "research"]
  "supported_environments": list[str], # ["production", "staging"]
}
```

### Supported OpenAI Parameters

```python
{
  "supported_openai_params": list[str],  # Parameters model accepts
}
```

**Common Values:**
```python
[
  # Generation Control
  "temperature", "max_tokens", "max_completion_tokens", "top_p",
  "frequency_penalty", "presence_penalty", "seed", "stop", "n",

  # Streaming
  "stream", "stream_options",

  # Tools/Functions
  "tools", "tool_choice", "function_call", "functions",
  "parallel_tool_calls",

  # Structured Outputs
  "response_format",

  # Advanced
  "logit_bias", "logprobs", "top_logprobs",
  "modalities", "prediction", "audio",
  "web_search_options", "service_tier",
  "reasoning_effort",  # For o1/o4 models

  # Metadata
  "user", "extra_headers", "max_retries"
]
```

### Custom Tokenizer

```python
{
  "custom_tokenizer": {
    "identifier": str,    # HuggingFace model ID
    "revision": str,      # Model revision/version
    "auth_token": str     # HF auth token for private models
  }
}
```

---

## Section 3: Common Patterns for Agents

### Pattern 1: Register Chat Model (OpenAI-compatible)

```json
{
  "model_name": "my-model",
  "litellm_params": {
    "model": "openai/my-model",
    "api_base": "http://localhost:8080/v1",
    "tags": ["custom", "local"]
  },
  "model_info": {
    "litellm_provider": "openai",
    "mode": "chat",
    "max_tokens": 8192,
    "max_input_tokens": 8192,
    "max_output_tokens": 4096,
    "input_cost_per_token": 0.0,
    "output_cost_per_token": 0.0,
    "supports_system_messages": true,
    "supports_function_calling": true,
    "supports_native_streaming": true
  }
}
```

### Pattern 2: Register Ollama Model (Native Mode)

```json
{
  "model_name": "prefix/llama3",
  "litellm_params": {
    "model": "ollama/llama3:8b",
    "api_base": "http://ollama:11434",
    "tags": ["lupdater", "provider:ollama", "type:ollama"]
  },
  "model_info": {
    "litellm_provider": "ollama",
    "mode": "chat",
    "max_tokens": 8192,
    "input_cost_per_token": 0.0,
    "output_cost_per_token": 0.0,
    "supports_system_messages": true,
    "supports_native_streaming": true
  }
}
```

### Pattern 3: Register Ollama Model (OpenAI Mode)

```json
{
  "model_name": "prefix/llama3",
  "litellm_params": {
    "model": "openai/llama3:8b",
    "api_base": "http://ollama:11434/v1",
    "tags": ["lupdater", "provider:ollama", "type:ollama"]
  },
  "model_info": {
    "litellm_provider": "openai",
    "mode": "chat",
    "max_tokens": 8192,
    "input_cost_per_token": 0.0,
    "output_cost_per_token": 0.0,
    "supports_system_messages": true,
    "supports_function_calling": false
  }
}
```

**Key Difference:**
- Native mode: `model: "ollama/..."`, `api_base` without `/v1`
- OpenAI mode: `model: "openai/..."`, `api_base` with `/v1` suffix

### Pattern 4: Register Embedding Model

```json
{
  "model_name": "my-embeddings",
  "litellm_params": {
    "model": "text-embedding-3-large"
  },
  "model_info": {
    "litellm_provider": "openai",
    "mode": "embedding",
    "max_input_tokens": 8191,
    "output_vector_size": 3072,
    "input_cost_per_token": 1.3e-07,
    "supported_openai_params": ["user", "encoding_format", "dimensions"]
  }
}
```

### Pattern 5: Register Vision Model

```json
{
  "model_name": "gpt-4-vision",
  "litellm_params": {
    "model": "gpt-4o"
  },
  "model_info": {
    "litellm_provider": "openai",
    "mode": "chat",
    "max_tokens": 128000,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384,
    "supports_system_messages": true,
    "supports_vision": true,
    "supports_function_calling": true,
    "supports_response_schema": true,
    "supports_prompt_caching": true
  }
}
```

### Pattern 6: Register Reasoning Model (o1/o4)

```json
{
  "model_name": "o4-mini",
  "litellm_params": {
    "model": "o4-mini",
    "merge_reasoning_content_in_choices": false
  },
  "model_info": {
    "litellm_provider": "openai",
    "mode": "responses",
    "max_tokens": 100000,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000,
    "supports_reasoning": true,
    "supports_system_messages": true,
    "supports_response_schema": true,
    "supports_prompt_caching": true,
    "supported_openai_params": ["reasoning_effort", "max_tokens", "temperature"]
  }
}
```

### Pattern 7: Azure OpenAI

```json
{
  "model_name": "azure-gpt4",
  "litellm_params": {
    "model": "azure/gpt-4o-deployment",
    "api_base": "https://my-resource.openai.azure.com",
    "api_key": "os.environ/AZURE_API_KEY",
    "api_version": "2024-02-15-preview"
  },
  "model_info": {
    "litellm_provider": "azure",
    "mode": "chat",
    "max_tokens": 128000,
    "access_groups": ["engineering"],
    "supported_environments": ["production"]
  }
}
```

---

## Section 4: Field Validation Rules

### Required Fields (Minimum Viable Model)

```python
REQUIRED = {
  "model_name": str,              # Must be unique
  "litellm_params.model": str,    # Must match provider pattern
  "model_info.litellm_provider": str,  # Must be valid provider
  "model_info.mode": str          # Must be valid mode
}
```

### Conditional Requirements

```python
# If using custom endpoint:
if "api_base" in litellm_params:
    # Usually need custom_llm_provider or prefix in model
    pass

# If mode == "embedding":
if model_info["mode"] == "embedding":
    # Should set output_vector_size
    # Should set max_input_tokens (not max_output_tokens)
    pass

# If mode == "audio_transcription" or "audio_speech":
if model_info["mode"] in ["audio_transcription", "audio_speech"]:
    # Use input_cost_per_second instead of input_cost_per_token
    pass

# If mode == "image_generation":
if model_info["mode"] == "image_generation":
    # Use output_cost_per_image
    pass
```

### Default Values

```python
DEFAULTS = {
  "litellm_params.use_in_pass_through": False,
  "litellm_params.use_litellm_proxy": False,
  "litellm_params.merge_reasoning_content_in_choices": False,

  "model_info.db_model": True,  # If added via API
  "model_info.input_cost_per_token": 0.0,  # For local models
  "model_info.output_cost_per_token": 0.0,
}
```

### Null vs Missing vs Zero

- **`null`**: Feature not applicable/not supported
- **Missing field**: Not configured, use LiteLLM defaults
- **`0` or `0.0`**: Explicitly set to zero (e.g., free local models)

---

## Section 5: API Usage Examples

### Add Model via API

```bash
curl -X POST http://localhost:4000/model/new \
  -H "Authorization: Bearer sk-1234" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "local-llama3",
    "litellm_params": {
      "model": "ollama/llama3:8b",
      "api_base": "http://ollama:11434",
      "tags": ["local", "ollama"]
    },
    "model_info": {
      "litellm_provider": "ollama",
      "mode": "chat",
      "max_tokens": 8192,
      "supports_system_messages": true
    }
  }'
```

### List All Models

```bash
curl -H "Authorization: Bearer sk-1234" \
  http://localhost:4000/model/info | jq '.data'
```

### Delete Model

```bash
# Get model UUID from /model/info first
curl -X POST http://localhost:4000/model/delete \
  -H "Authorization: Bearer sk-1234" \
  -H "Content-Type: application/json" \
  -d '{
    "id": "ecb9968d-7bdf-4a4e-ac78-0c39e301b2d1"
  }'
```

---

## Section 6: Complete Field Reference

### litellm_params (Complete List)

```python
{
  # Connection
  "model": str,
  "custom_llm_provider": str,
  "api_base": str,
  "api_key": str,
  "api_version": str,

  # Behavior
  "use_in_pass_through": bool,
  "use_litellm_proxy": bool,
  "merge_reasoning_content_in_choices": bool,

  # Generation
  "temperature": float,
  "max_tokens": int,
  "max_completion_tokens": int,
  "top_p": float,
  "frequency_penalty": float,
  "presence_penalty": float,
  "seed": int,
  "n": int,
  "stop": str | list[str],

  # Tools/Functions
  "tools": list[dict],
  "tool_choice": str | dict,
  "function_call": str | dict,
  "functions": list[dict],
  "parallel_tool_calls": bool,

  # Structured Output
  "response_format": dict,

  # Streaming
  "stream": bool,
  "stream_options": dict,

  # Advanced
  "logit_bias": dict,
  "logprobs": bool,
  "top_logprobs": int,
  "modalities": list[str],
  "prediction": dict,
  "audio": dict,
  "web_search_options": dict,
  "reasoning_effort": str,

  # Rate Limiting
  "rpm": int,
  "tpm": int,

  # Auth
  "organization": str,
  "azure_ad_token": str,
  "aws_region_name": str,

  # HTTP
  "extra_headers": dict,
  "max_retries": int,

  # Metadata
  "service_tier": str,
  "safety_identifier": str,
  "user": str,

  # Organization
  "tags": list[str]
}
```

### model_info (Complete List)

```python
{
  # Identity
  "id": str,
  "db_model": bool,
  "key": str,
  "version": str,

  # Provider
  "litellm_provider": str,
  "mode": str,

  # Tokens
  "max_tokens": int,
  "max_input_tokens": int,
  "max_output_tokens": int,
  "context_window": int,

  # Capabilities (16 fields)
  "supports_system_messages": bool,
  "supports_response_schema": bool,
  "supports_vision": bool,
  "supports_function_calling": bool,
  "supports_tool_choice": bool,
  "supports_assistant_prefill": bool,
  "supports_prompt_caching": bool,
  "supports_audio_input": bool,
  "supports_audio_output": bool,
  "supports_pdf_input": bool,
  "supports_embedding_image_input": bool,
  "supports_native_streaming": bool,
  "supports_web_search": bool,
  "supports_url_context": bool,
  "supports_reasoning": bool,
  "supports_computer_use": bool,

  # Input Costs (10 fields)
  "input_cost_per_token": float,
  "input_cost_per_token_flex": float,
  "input_cost_per_token_priority": float,
  "input_cost_per_token_above_128k_tokens": float,
  "input_cost_per_token_above_200k_tokens": float,
  "input_cost_per_token_batches": float,
  "input_cost_per_character": float,
  "input_cost_per_second": float,
  "input_cost_per_query": float,
  "input_cost_per_audio_token": float,

  # Cache Costs (7 fields)
  "cache_creation_input_token_cost": float,
  "cache_creation_input_token_cost_above_200k_tokens": float,
  "cache_creation_input_token_cost_above_1hr": float,
  "cache_read_input_token_cost": float,
  "cache_read_input_token_cost_above_200k_tokens": float,
  "cache_read_input_token_cost_flex": float,
  "cache_read_input_token_cost_priority": float,

  # Output Costs (14 fields)
  "output_cost_per_token": float,
  "output_cost_per_token_flex": float,
  "output_cost_per_token_priority": float,
  "output_cost_per_token_above_128k_tokens": float,
  "output_cost_per_token_above_200k_tokens": float,
  "output_cost_per_token_batches": float,
  "output_cost_per_character": float,
  "output_cost_per_character_above_128k_tokens": float,
  "output_cost_per_reasoning_token": float,
  "output_cost_per_audio_token": float,
  "output_cost_per_second": float,
  "output_cost_per_image": float,
  "output_cost_per_video_per_second": float,

  # Other Costs (5 fields)
  "search_context_cost_per_query": float,
  "citation_cost_per_token": float,
  "ocr_cost_per_page": float,
  "annotation_cost_per_page": float,
  "tiered_pricing": dict,

  # Other
  "output_vector_size": int,
  "tpm": int,
  "rpm": int,
  "supported_openai_params": list[str],
  "access_groups": list[str],
  "supported_environments": list[str],
  "custom_tokenizer": {
    "identifier": str,
    "revision": str,
    "auth_token": str
  }
}
```

---

## Section 7: Decision Trees for Agents

### Choosing litellm_provider

```
Is it Ollama in native mode?
├─ Yes → litellm_provider: "ollama"
│         model: "ollama/model-name"
│         api_base: without /v1
└─ No
   ├─ Is it Ollama in OpenAI-compatible mode?
   │  ├─ Yes → litellm_provider: "openai"
   │  │         model: "openai/model-name"
   │  │         api_base: with /v1 suffix
   │  └─ No
   │     ├─ Is it Azure?
   │     │  ├─ Yes → litellm_provider: "azure"
   │     │  │         model: "azure/deployment-name"
   │     │  └─ No
   │     │     ├─ Is it Anthropic?
   │     │     │  ├─ Yes → litellm_provider: "anthropic"
   │     │     │  └─ No
   │     │     │     ├─ Is it AWS Bedrock?
   │     │     │     │  ├─ Yes → litellm_provider: "bedrock"
   │     │     │     │  └─ No
   │     │     │     │     └─ Default → litellm_provider: "openai"
```

### Choosing mode

```
What does the model do?
├─ Chat/conversation → mode: "chat"
├─ Text completion → mode: "completion"
├─ Embeddings → mode: "embedding"
├─ Audio transcription → mode: "audio_transcription"
├─ Text-to-speech → mode: "audio_speech"
├─ Image generation → mode: "image_generation"
├─ Content moderation → mode: "moderation"
└─ Advanced reasoning (o1/o4) → mode: "responses"
```

### Setting Costs

```
Is the model local/free?
├─ Yes → input_cost_per_token: 0.0
│         output_cost_per_token: 0.0
└─ No
   ├─ Is it an audio model?
   │  ├─ Yes → Use input_cost_per_second / output_cost_per_second
   │  └─ No
   │     ├─ Is it TTS?
   │     │  ├─ Yes → Use input_cost_per_character
   │     │  └─ No
   │     │     ├─ Is it image generation?
   │     │     │  ├─ Yes → Use output_cost_per_image
   │     │     │  └─ No
   │     │     │     └─ Default → Use input_cost_per_token / output_cost_per_token
```

---

## Section 8: Real-World Examples from Production

### Example 1: GPT-4o (Full Production Config)

```json
{
  "model_name": "gpt-4o",
  "litellm_params": {
    "model": "gpt-4o",
    "custom_llm_provider": "openai"
  },
  "model_info": {
    "id": "e40230fa-b469-47ab-92c0-1e9b33c0cbae",
    "db_model": true,
    "key": "gpt-4o",
    "max_tokens": 128000,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384,
    "input_cost_per_token": 2.5e-06,
    "output_cost_per_token": 1e-05,
    "cache_read_input_token_cost": 1.25e-06,
    "input_cost_per_token_batches": 1.25e-06,
    "output_cost_per_token_batches": 5e-06,
    "litellm_provider": "openai",
    "mode": "chat",
    "supports_system_messages": true,
    "supports_response_schema": true,
    "supports_vision": true,
    "supports_function_calling": true,
    "supports_tool_choice": true,
    "supports_prompt_caching": true,
    "supports_audio_input": true,
    "supports_audio_output": true,
    "supports_pdf_input": true,
    "supports_native_streaming": true,
    "supported_openai_params": [
      "frequency_penalty", "logit_bias", "logprobs", "top_logprobs",
      "max_tokens", "max_completion_tokens", "modalities", "prediction",
      "n", "presence_penalty", "seed", "stop", "stream", "stream_options",
      "temperature", "top_p", "tools", "tool_choice", "function_call",
      "functions", "max_retries", "extra_headers", "parallel_tool_calls",
      "audio", "web_search_options", "service_tier", "safety_identifier",
      "response_format", "user"
    ]
  }
}
```

### Example 2: Text Embedding 3 Large

```json
{
  "model_name": "text-embedding-3-large",
  "litellm_params": {
    "model": "text-embedding-3-large",
    "custom_llm_provider": "openai"
  },
  "model_info": {
    "id": "5314a8d1-1825-40b7-b264-ab838cea3e34",
    "db_model": true,
    "key": "text-embedding-3-large",
    "max_tokens": 8191,
    "max_input_tokens": 8191,
    "max_output_tokens": null,
    "input_cost_per_token": 1.3e-07,
    "input_cost_per_token_batches": 6.5e-08,
    "output_cost_per_token": 0.0,
    "output_vector_size": 3072,
    "litellm_provider": "openai",
    "mode": "embedding"
  }
}
```

### Example 3: DALL-E 3

```json
{
  "model_name": "dall-e-3",
  "litellm_params": {
    "model": "dall-e-3",
    "custom_llm_provider": "openai"
  },
  "model_info": {
    "id": "561c6cb2-0cff-41fe-97f6-a30c3ecfc0d1",
    "db_model": true,
    "key": "dall-e-3",
    "max_tokens": null,
    "max_input_tokens": null,
    "max_output_tokens": null,
    "input_cost_per_token": 0.0,
    "output_cost_per_token": 0.0,
    "output_cost_per_image": 0.04,
    "litellm_provider": "openai",
    "mode": "image_generation",
    "supported_openai_params": [
      "quality", "n", "size", "style", "response_format", "user"
    ]
  }
}
```

### Example 4: Whisper-1

```json
{
  "model_name": "whisper-1",
  "litellm_params": {
    "model": "whisper-1",
    "custom_llm_provider": "openai"
  },
  "model_info": {
    "id": "ecb9968d-7bdf-4a4e-ac78-0c39e301b2d1",
    "db_model": true,
    "key": "whisper-1",
    "max_tokens": null,
    "input_cost_per_second": 0.0001,
    "output_cost_per_second": 0.0001,
    "litellm_provider": "openai",
    "mode": "audio_transcription"
  }
}
```

### Example 5: o4-mini (Reasoning Model)

```json
{
  "model_name": "o4-mini",
  "litellm_params": {
    "model": "o4-mini",
    "custom_llm_provider": "openai",
    "merge_reasoning_content_in_choices": false
  },
  "model_info": {
    "id": "180f8850-6ac7-4683-b2e7-b775493282e4",
    "db_model": true,
    "key": "o4-mini",
    "max_tokens": 100000,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000,
    "input_cost_per_token": 1.1e-06,
    "input_cost_per_token_flex": 5.5e-07,
    "input_cost_per_token_priority": 2e-06,
    "cache_read_input_token_cost": 2.75e-07,
    "cache_read_input_token_cost_flex": 1.375e-07,
    "cache_read_input_token_cost_priority": 5e-07,
    "output_cost_per_token": 4.4e-06,
    "output_cost_per_token_flex": 2.2e-06,
    "output_cost_per_token_priority": 8e-06,
    "litellm_provider": "openai",
    "mode": "responses",
    "supports_system_messages": true,
    "supports_response_schema": true,
    "supports_vision": true,
    "supports_function_calling": true,
    "supports_tool_choice": true,
    "supports_prompt_caching": true,
    "supports_pdf_input": true,
    "supports_native_streaming": true,
    "supports_reasoning": true,
    "supported_openai_params": [
      "logit_bias", "max_tokens", "max_completion_tokens",
      "modalities", "prediction", "n", "seed", "stop",
      "stream", "stream_options", "temperature", "tools",
      "tool_choice", "function_call", "functions",
      "max_retries", "extra_headers", "audio",
      "web_search_options", "service_tier",
      "safety_identifier", "response_format", "user",
      "reasoning_effort"
    ]
  }
}
```

---

## Section 9: Common Mistakes to Avoid

### Mistake 1: Wrong Provider for Ollama

```python
# WRONG - Ollama native mode with openai provider
{
  "litellm_params": {
    "model": "ollama/llama3",
    "api_base": "http://ollama:11434"
  },
  "model_info": {
    "litellm_provider": "openai"  # ❌ Should be "ollama"
  }
}

# CORRECT - Ollama native mode
{
  "litellm_params": {
    "model": "ollama/llama3",
    "api_base": "http://ollama:11434"
  },
  "model_info": {
    "litellm_provider": "ollama"  # ✓
  }
}
```

### Mistake 2: Missing /v1 for OpenAI Mode

```python
# WRONG - OpenAI-compatible mode without /v1
{
  "litellm_params": {
    "model": "openai/llama3",
    "api_base": "http://ollama:11434"  # ❌ Missing /v1
  }
}

# CORRECT
{
  "litellm_params": {
    "model": "openai/llama3",
    "api_base": "http://ollama:11434/v1"  # ✓
  }
}
```

### Mistake 3: Wrong Token Limits for Embeddings

```python
# WRONG - Embedding with max_output_tokens
{
  "model_info": {
    "mode": "embedding",
    "max_output_tokens": 1536  # ❌ Embeddings don't have output tokens
  }
}

# CORRECT
{
  "model_info": {
    "mode": "embedding",
    "max_input_tokens": 8191,
    "output_vector_size": 1536  # ✓
  }
}
```

### Mistake 4: Wrong Cost Fields for Audio

```python
# WRONG - Audio model with token costs
{
  "model_info": {
    "mode": "audio_transcription",
    "input_cost_per_token": 0.0001  # ❌ Should use seconds
  }
}

# CORRECT
{
  "model_info": {
    "mode": "audio_transcription",
    "input_cost_per_second": 0.0001  # ✓
  }
}
```

### Mistake 5: Forgetting Tags for Organization

```python
# LESS USEFUL - No tags
{
  "litellm_params": {
    "model": "ollama/llama3"
  }
}

# BETTER - With tags for filtering
{
  "litellm_params": {
    "model": "ollama/llama3",
    "tags": ["lupdater", "provider:ollama", "type:chat", "env:prod"]
  }
}
```

---

## Section 10: Quick Lookup Tables

### Model Mode by Use Case

| Use Case | Mode | Key Fields |
|----------|------|------------|
| Chatbot | `chat` | max_tokens, supports_function_calling |
| RAG embeddings | `embedding` | output_vector_size, max_input_tokens |
| Speech-to-text | `audio_transcription` | input_cost_per_second |
| Text-to-speech | `audio_speech` | input_cost_per_character |
| Image generation | `image_generation` | output_cost_per_image |
| Content filtering | `moderation` | max_input_tokens |
| Complex reasoning | `responses` | supports_reasoning, reasoning_effort |

### Provider Patterns

| Provider | Model Pattern | API Base Pattern | Notes |
|----------|--------------|------------------|-------|
| OpenAI | `gpt-4` | Default | Native OpenAI |
| Azure | `azure/deployment` | `https://*.openai.azure.com` | Needs api_version |
| Anthropic | `claude-3-opus` | Default | Native Anthropic |
| Ollama (native) | `ollama/model:tag` | `http://host:11434` | No /v1 |
| Ollama (OpenAI) | `openai/model:tag` | `http://host:11434/v1` | With /v1 |
| Bedrock | `bedrock/anthropic.claude` | N/A | AWS credentials |
| Custom OpenAI | `openai/model` | `http://host:port/v1` | Any OpenAI-compatible |

### Cost Field Selection

| Billing Unit | Input Field | Output Field | Use Case |
|--------------|-------------|--------------|----------|
| Tokens | input_cost_per_token | output_cost_per_token | Chat, completion |
| Characters | input_cost_per_character | - | TTS |
| Seconds | input_cost_per_second | output_cost_per_second | Audio |
| Images | - | output_cost_per_image | DALL-E |
| Fixed | input_cost_per_query | - | Some APIs |

---

## Data Sources

1. **Live LiteLLM Instance:** http://localhost:4000/model/info (17 models analyzed)
2. **Official Docs:** https://docs.litellm.ai/docs/proxy/configs
3. **Model Management:** https://docs.litellm.ai/docs/proxy/model_management
4. **Input Parameters:** https://docs.litellm.ai/docs/completion/input

---

**Document Version:** 2.0 (Agent-Optimized)
**Last Updated:** 2025-11-29
**Maintainer:** LiteLLM Companion Project
